{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpelm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.layers.core import Dense, Flatten,Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import plotly\n",
    "from plotly.graph_objs import Layout, Figure, Marker\n",
    "import time\n",
    "CNN_EPOCH = 10\n",
    "image_shape=(224,224,3)\n",
    "NUM_CLASS = 2\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "# metric\n",
    "from keras.metrics import binary_crossentropy\n",
    "# optimization method (Stochastic Gradient Descent (SGD))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Train IELM and evaluating training accuracy\n",
    "no_classes = 2\n",
    "error = 0.1\n",
    "loss_function = \"binary_cross_entropy\" #\"mean_squared_error\"  #It can be mean_absolute_error also\n",
    "activation_function = \"tanh\"\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(train_path,test_path):\n",
    "\n",
    "    # Define the data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to (224, 224)\n",
    "        transforms.ToTensor(),  # Convert images to tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "    ])\n",
    " \n",
    "\n",
    "    train_dataset = ImageFolder(train_path, transform=transform)\n",
    "    traindata_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "    test_dataset = ImageFolder(test_path, transform=transform)\n",
    "    testdata_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "    return traindata_loader,testdata_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model():\n",
    "    #loading of model\n",
    "    device=torch.device(\"cuda\")  \n",
    "    \n",
    "    base_model = models.alexnet(pretrained=True)\n",
    "    base_model = torch.nn.Sequential(*list(base_model.children())[:-1])\n",
    "    base_model=base_model.to(device)\n",
    "    base_model.eval()\n",
    "    return base_model\n",
    "#training features extraction\n",
    "def training_feature_extraction(device, traindata_loader,resnet_model):\n",
    "\n",
    "    x_training_features = []\n",
    "    y_train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in traindata_loader:        \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Extract features using VGG16\n",
    "            with torch.no_grad():\n",
    "                features = resnet_model(images)   \n",
    "            \n",
    "            x_training_features.append(features)\n",
    "            y_train_labels.append(labels)\n",
    "    x_training_features = torch.cat(x_training_features, dim=0)\n",
    "    y_train_labels = torch.cat(y_train_labels, dim=0)\n",
    "\n",
    "    x_training_features.to(device)\n",
    "    y_train_labels.to(device)\n",
    "    return x_training_features,y_train_labels\n",
    "\n",
    "def testing_feature_extratcion(device,testdata_loader,vgg_model):\n",
    "    x_testing_features = []\n",
    "    y_testing_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testdata_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Extract features using VGG19\n",
    "            with torch.no_grad():\n",
    "                features = vgg_model(images) \n",
    "            x_testing_features.append(vgg_model(images))\n",
    "            y_testing_labels.append(labels)\n",
    "    x_testing_features = torch.cat(x_testing_features, dim=0)\n",
    "    y_testing_labels = torch.cat(y_testing_labels, dim=0)\n",
    "\n",
    "    x_testing_features.to(device)\n",
    "    y_testing_labels.to(device)\n",
    "    return x_testing_features,y_testing_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_features_conversion(x_training_features,y_train_labels):\n",
    "\n",
    "    # Convert the extracted features to numpy arrays\n",
    "\n",
    "    device=torch.device(\"cuda\")\n",
    "    x_training_features_np = x_training_features.detach().cpu().numpy()  \n",
    "    flattened_features_train = x_training_features_np.reshape(x_training_features_np.shape[0], -1)\n",
    "    train_features_tensor = torch.tensor(flattened_features_train, dtype=torch.float32).to(device)\n",
    "    \n",
    "    y_train_array= y_train_labels.cpu().detach().numpy()\n",
    "    y_train_array_encoded = np_utils.to_categorical(y_train_array,no_classes)\n",
    "    y_train_labels_encoded=torch.tensor(y_train_array_encoded,dtype=torch.float32)\n",
    "    y_train_labels_encoded=y_train_labels_encoded.to(device)    \n",
    "    return train_features_tensor,y_train_labels_encoded\n",
    "\n",
    "def test_features_conversion(x_testing_features,y_testing_labels):\n",
    "    # Convert the extracted features to numpy arrays\n",
    "    device=torch.device(\"cuda\")\n",
    "    x_testing_features_np = x_testing_features.detach().cpu().numpy()\n",
    "    \n",
    "    flattened_features_test = x_testing_features_np.reshape(x_testing_features_np.shape[0], -1)\n",
    "    test_features_tensor = torch.tensor(flattened_features_test,dtype=torch.float32).to(device)\n",
    "    \n",
    "    y_test_array= y_testing_labels.cpu().detach().numpy()\n",
    "    y_test_array_encoded = np_utils.to_categorical(y_test_array,no_classes)\n",
    "    y_test_labels_encoded=torch.tensor(y_test_array_encoded,dtype=torch.float32)\n",
    "    y_test_labels_encoded=y_test_labels_encoded.to(device)\n",
    "    return test_features_tensor, y_test_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")\n",
    "train_path=\"D:/ALEXELM/Dehazed_Images_3400/Dehazed_Images/train/\"\n",
    "test_path=\"D:/ALEXELM/Dehazed_Images_3400/Dehazed_Images/test/\"\n",
    "traindata_loader,testdata_loader=loading_data(train_path,test_path)\n",
    "model=load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_features,y_train_labels=training_feature_extraction(device,traindata_loader,model)\n",
    "train_features_tensor,y_train_labels_encoded=train_features_conversion(x_training_features,y_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_features_tensor,\"alexnet_dehazed_train_features.pth\")\n",
    "torch.save(y_train_labels_encoded,\"alexnet_dehazed_train_labels.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_features,y_testing_labels=testing_feature_extratcion(device,testdata_loader,model)\n",
    "test_features_tensor, y_test_labels_encoded=test_features_conversion(x_testing_features,y_testing_labels)\n",
    "torch.save(test_features_tensor,\"alexnet_dehazed_test_features.pth\")\n",
    "torch.save(y_test_labels_encoded,\"alexnet_dehazed_test_labels.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device= torch.device(\"cuda\")\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used for training.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. CPU will be used for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for selection of best parameters\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "def best_parameter():\n",
    "# Wrap the model using KerasClassifier\n",
    "    keras_model = KerasClassifier(build_fn=Alexnet, verbose=0)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'optimizer': ['adam', 'rmsprop','SGD'],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        # Add more hyperparameters to tune as needed\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "    #y_train_max= np.array([np.argmax(r) for r in y_train])\n",
    "    # Fit the grid search to the data\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters found\n",
    "    print(\"Best Parameters:\", grid_result.best_params_)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_result.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_test_max= np.array([np.argmax(r) for r in y_test])\n",
    "accuracy = accuracy_score(y_test_max, y_pred)\n",
    "print(\"Test Set Accuracy:\", accuracy)\n",
    "device=torch.device(\"cuda\") \n",
    "train_history, acc ,alexmodel= train_model(base_model, traindata_loader, testdata_loader, epochs =10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation of features\n",
    "from keras.models import load_model,Model\n",
    "#from keras.preprocessing.image import load_images_and_labels\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "\n",
    "# Load an image from a file\n",
    "#img = load_img('path_to_image.jpg', target_size=(224, 224))\n",
    "vm=load_model(\"alex_model_10.h5\")\n",
    "\n",
    "cnnModel=Sequential()\n",
    "cnnModel=Model(inputs=vm.inputs, outputs=vm.layers[6].output)\n",
    "cnnModel.summary()\n",
    "#for i in range(10):\n",
    "print(vm.layers[0].output)\n",
    "print(vm.layers[2].output)\n",
    "print(vm.layers[4].output)\n",
    "print(vm.layers[5].output)\n",
    "print(vm.layers[6].output)\n",
    "#print(vm.layers[8].output)\n",
    "\n",
    "#vm.layers[1].\n",
    "img=load_img(\"D:/ALEXELM/Dehazed_Images_3400/Dehazed_Images/train/busy/(35).jpg\",target_size=(227,227,3))\n",
    "img_arr=img_to_array(img)\n",
    "img_arr=np.expand_dims(img_arr,axis=0)\n",
    "\n",
    "feature_maps=cnnModel.predict(img_arr)\n",
    "square,ix=4,1\n",
    "from matplotlib import pyplot as plt\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        ax=plt.subplot(square,square,ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(feature_maps[0,:,:,ix-1],cmap='pink')\n",
    "        ix+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_features=torch.load(\"alexnet_dehazed_train_features.pth\")\n",
    "y_train_labels=torch.load(\"alexnet_dehazed_train_labels.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_features=torch.load(\"alexnet_PUC_test_features.pth\")\n",
    "y_testing_labels=torch.load(\"alexnet_PUC_test_labels.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_labels_reshaped=np.argmax(y_train_labels.detach().cpu().numpy(),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels_reshaped=np.argmax(y_testing_labels.detach().cpu().numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "def featureselection_rf(x_training_features,y_train_labels_reshaped):\n",
    "    # Train a Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(x_training_features.cpu().numpy(), y_train_labels_reshaped)\n",
    "\n",
    "    # Get feature importance scores\n",
    "    importance_scores = rf.feature_importances_\n",
    "\n",
    "    # Sort features based on importance scores\n",
    "    sorted_indices = np.argsort(importance_scores)[::-1]\n",
    "\n",
    "\n",
    "    # Select top k features (e.g., top 2 features)\n",
    "    k = 10\n",
    "    selected_features = sorted_indices[:k]\n",
    "\n",
    "    # Print selected features and their importance scores\n",
    "    for i in selected_features:\n",
    "        print(f\"Feature {i}: Importance Score = {importance_scores[i]}\")\n",
    "\n",
    "    \n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************ML_ELM********************\n",
    "class ML_ELM_:\n",
    "    def __init__(self, l1, l2, l3,l4):\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.l3 = l3\n",
    "        self.l4 = l4\n",
    "        pass\n",
    "\n",
    "        def hidden_nodes(X, input_weights, biases):\n",
    "            from numpy import dot\n",
    "            def relu(x):\n",
    "                from numpy import maximum\n",
    "                return maximum(x, 0, x)\n",
    "\n",
    "            G = dot(X, input_weights)\n",
    "            G = G + biases\n",
    "            H = relu(G)\n",
    "            return H\n",
    "\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "\n",
    "        def ELM_hidden(train_in, input_weights):\n",
    "            biases = 0\n",
    "            hidden = hidden_nodes(train_in, input_weights, biases)\n",
    "\n",
    "            return hidden\n",
    "\n",
    "        self.ELM_hidden = ELM_hidden\n",
    "\n",
    "    def fit(self, train_in_, train_out):\n",
    "        def ELM_weights(train_in, hidden_size):\n",
    "            from scipy.linalg import pinv\n",
    "            from numpy import random\n",
    "            from numpy import dot\n",
    "            from numpy import transpose\n",
    "\n",
    "            input_size = train_in.shape[1]\n",
    "            hidden_size = int(hidden_size)\n",
    "            input_weights = random.normal(size=[input_size, hidden_size])\n",
    "            biases = random.normal(size=[hidden_size])\n",
    "\n",
    "            output_weights = dot(pinv(self.hidden_nodes(train_in, input_weights, biases)), train_in)\n",
    "            transposed_weights = transpose(output_weights)\n",
    "\n",
    "            return transposed_weights\n",
    "\n",
    "        def ELM_train(train_in, train_out):\n",
    "            from scipy.linalg import pinv\n",
    "            from numpy import dot\n",
    "\n",
    "            output_weights = dot(pinv(train_in), train_out)\n",
    "\n",
    "            return output_weights\n",
    "\n",
    "        self.layers = [i for i in list([self.l1, self.l2, self.l3, self.l4]) if i != 0]\n",
    "        #self.layers = [i for i in list([self.l1, self.l2, self.l3]) if i != 0]\n",
    "        input_layers_tr = [train_in_]\n",
    "\n",
    "        self.weights = []\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.weights += [ELM_weights(input_layers_tr[i], layer)]\n",
    "            input_layers_tr += [self.ELM_hidden(input_layers_tr[i], self.weights[i])]\n",
    "\n",
    "        self.weights += [ELM_train(input_layers_tr[-1], train_out)]\n",
    "        return self\n",
    "\n",
    "    def predict(self, train_in_):\n",
    "        def predict(input, output_weights):\n",
    "            from numpy import dot\n",
    "            out = dot(input, output_weights)\n",
    "            return out\n",
    "\n",
    "        def ELM_predict(input, output_weights):\n",
    "            prediction = predict(input, output_weights)\n",
    "            return prediction\n",
    "\n",
    "        input_layers_tr = [train_in_]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            input_layers_tr += [self.ELM_hidden(input_layers_tr[i], self.weights[i])]\n",
    "        output_train_pred = ELM_predict(input_layers_tr[-1], self.weights[-1])\n",
    "        #print(output_train_pred)\n",
    "        return output_train_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9f79f829382866278eada64dff3ff002f65deb882a3977d3dc9813be743fb22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
